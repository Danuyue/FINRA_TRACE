{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder \"C:\\Users\\raymo\\AppData\\Local\\Temp\" will be used to save temporary dictionary and corpus.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-15 11:48:32,644 : INFO : 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('retina')\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "import tempfile\n",
    "TEMP_FOLDER = tempfile.gettempdir()\n",
    "print('Folder \"{}\" will be used to save temporary dictionary and corpus.'.format(TEMP_FOLDER))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the pandas we got from data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = Path('../Data/Pickle/')\n",
    "file_name = 'TRACE2014_jinming_pickle'\n",
    "file_path = root_folder / file_name\n",
    "\n",
    "data = pd.read_pickle(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"matrix_1_doc_list.txt\", \"rb\") as fp:\n",
    "     matrix_1_doc_list = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare documents"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "documents = data['document'].values\n",
    "documents = list(documents)\n",
    "\n",
    "texts = [[word for word in document.lower().split(',')]for document in documents]\n",
    "\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "\n",
    "#from pprint import pprint  # pretty-printer\n",
    "#pprint(texts)\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "#dictionary.save(os.path.join('Dict/', 'matrix_1_test.dict'))  # store the dictionary, for future reference\n",
    "print(dictionary)\n",
    "\n",
    "#print(dictionary.token2id)\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize(os.path.join(TEMP_FOLDER, 'deerwester.mm'), corpus)  # store to disk, for later use\n",
    "for c in corpus:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-15 11:53:51,492 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-11-15 11:53:51,539 : INFO : built Dictionary(264 unique tokens: ['0', '2014-07-16', '2014-12-23', '83', '2014-12-05']...) from 5000 documents (total 9999 corpus positions)\n",
      "2018-11-15 11:53:51,570 : WARNING : no word id mapping provided; initializing from corpus, assuming identity\n",
      "2018-11-15 11:53:51,570 : INFO : using symmetric alpha at 0.002\n",
      "2018-11-15 11:53:51,570 : INFO : using symmetric eta at 0.002\n",
      "2018-11-15 11:53:51,570 : INFO : using serial LDA version on this node\n",
      "2018-11-15 11:53:51,601 : INFO : running online (single-pass) LDA training, 500 topics, 1 passes over the supplied corpus of 5000 documents, updating model once every 2000 documents, evaluating perplexity every 5000 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2018-11-15 11:53:51,601 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "2018-11-15 11:53:51,601 : INFO : PROGRESS: pass 0, at document #2000/5000\n",
      "2018-11-15 11:53:52,210 : INFO : merging changes from 2000 documents into a model of 5000 documents\n",
      "2018-11-15 11:53:52,257 : INFO : topic #74 (0.002): 0.004*\"174\" + 0.004*\"173\" + 0.004*\"179\" + 0.004*\"178\" + 0.004*\"177\" + 0.004*\"176\" + 0.004*\"175\" + 0.004*\"181\" + 0.004*\"170\" + 0.004*\"169\"\n",
      "2018-11-15 11:53:52,257 : INFO : topic #89 (0.002): 0.004*\"174\" + 0.004*\"173\" + 0.004*\"179\" + 0.004*\"178\" + 0.004*\"177\" + 0.004*\"176\" + 0.004*\"175\" + 0.004*\"181\" + 0.004*\"170\" + 0.004*\"169\"\n",
      "2018-11-15 11:53:52,257 : INFO : topic #299 (0.002): 0.004*\"174\" + 0.004*\"173\" + 0.004*\"179\" + 0.004*\"178\" + 0.004*\"177\" + 0.004*\"176\" + 0.004*\"175\" + 0.004*\"181\" + 0.004*\"170\" + 0.004*\"169\"\n",
      "2018-11-15 11:53:52,257 : INFO : topic #349 (0.002): 0.004*\"174\" + 0.004*\"173\" + 0.004*\"179\" + 0.004*\"178\" + 0.004*\"177\" + 0.004*\"176\" + 0.004*\"175\" + 0.004*\"181\" + 0.004*\"170\" + 0.004*\"169\"\n",
      "2018-11-15 11:53:52,257 : INFO : topic #380 (0.002): 0.004*\"174\" + 0.004*\"173\" + 0.004*\"179\" + 0.004*\"178\" + 0.004*\"177\" + 0.004*\"176\" + 0.004*\"175\" + 0.004*\"181\" + 0.004*\"170\" + 0.004*\"169\"\n",
      "2018-11-15 11:53:52,257 : INFO : topic diff=492.579163, rho=1.000000\n",
      "2018-11-15 11:53:52,257 : INFO : PROGRESS: pass 0, at document #4000/5000\n",
      "C:\\Users\\raymo\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py:775: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n",
      "2018-11-15 11:53:52,992 : INFO : merging changes from 2000 documents into a model of 5000 documents\n",
      "2018-11-15 11:53:53,038 : INFO : topic #19 (0.002): 0.004*\"174\" + 0.004*\"173\" + 0.004*\"179\" + 0.004*\"178\" + 0.004*\"177\" + 0.004*\"176\" + 0.004*\"175\" + 0.004*\"181\" + 0.004*\"170\" + 0.004*\"169\"\n",
      "2018-11-15 11:53:53,038 : INFO : topic #480 (0.002): 0.004*\"174\" + 0.004*\"173\" + 0.004*\"179\" + 0.004*\"178\" + 0.004*\"177\" + 0.004*\"176\" + 0.004*\"175\" + 0.004*\"181\" + 0.004*\"170\" + 0.004*\"169\"\n",
      "2018-11-15 11:53:53,038 : INFO : topic #408 (0.002): 0.004*\"174\" + 0.004*\"173\" + 0.004*\"179\" + 0.004*\"178\" + 0.004*\"177\" + 0.004*\"176\" + 0.004*\"175\" + 0.004*\"181\" + 0.004*\"170\" + 0.004*\"169\"\n",
      "2018-11-15 11:53:53,054 : INFO : topic #4 (0.002): 0.004*\"174\" + 0.004*\"173\" + 0.004*\"179\" + 0.004*\"178\" + 0.004*\"177\" + 0.004*\"176\" + 0.004*\"175\" + 0.004*\"181\" + 0.004*\"170\" + 0.004*\"169\"\n",
      "2018-11-15 11:53:53,054 : INFO : topic #347 (0.002): 0.004*\"174\" + 0.004*\"173\" + 0.004*\"179\" + 0.004*\"178\" + 0.004*\"177\" + 0.004*\"176\" + 0.004*\"175\" + 0.004*\"181\" + 0.004*\"170\" + 0.004*\"169\"\n",
      "2018-11-15 11:53:53,054 : INFO : topic diff=inf, rho=0.707107\n",
      "2018-11-15 11:53:53,491 : INFO : -992.524 per-word bound, 60203267310455595873494909968637870604476458554920426669727858477320193645805134608001924348256080327450719501628990067496716352323798503099365253809540005685794897761320420181538197548660934242986474758478241508220689534642452138971866592858283533290845889150654031808445033261191376984368230694912.0 perplexity estimate based on a held-out corpus of 1000 documents with 2000 words\n",
      "2018-11-15 11:53:53,491 : INFO : PROGRESS: pass 0, at document #5000/5000\n",
      "2018-11-15 11:53:53,851 : INFO : merging changes from 1000 documents into a model of 5000 documents\n",
      "2018-11-15 11:53:53,898 : INFO : topic #481 (0.002): 0.004*\"174\" + 0.004*\"173\" + 0.004*\"179\" + 0.004*\"178\" + 0.004*\"177\" + 0.004*\"176\" + 0.004*\"175\" + 0.004*\"181\" + 0.004*\"170\" + 0.004*\"169\"\n",
      "2018-11-15 11:53:53,898 : INFO : topic #68 (0.002): 0.004*\"174\" + 0.004*\"173\" + 0.004*\"179\" + 0.004*\"178\" + 0.004*\"177\" + 0.004*\"176\" + 0.004*\"175\" + 0.004*\"181\" + 0.004*\"170\" + 0.004*\"169\"\n",
      "2018-11-15 11:53:53,898 : INFO : topic #430 (0.002): 0.004*\"174\" + 0.004*\"173\" + 0.004*\"179\" + 0.004*\"178\" + 0.004*\"177\" + 0.004*\"176\" + 0.004*\"175\" + 0.004*\"181\" + 0.004*\"170\" + 0.004*\"169\"\n",
      "2018-11-15 11:53:53,898 : INFO : topic #50 (0.002): 0.004*\"174\" + 0.004*\"173\" + 0.004*\"179\" + 0.004*\"178\" + 0.004*\"177\" + 0.004*\"176\" + 0.004*\"175\" + 0.004*\"181\" + 0.004*\"170\" + 0.004*\"169\"\n",
      "2018-11-15 11:53:53,898 : INFO : topic #28 (0.002): 0.004*\"174\" + 0.004*\"173\" + 0.004*\"179\" + 0.004*\"178\" + 0.004*\"177\" + 0.004*\"176\" + 0.004*\"175\" + 0.004*\"181\" + 0.004*\"170\" + 0.004*\"169\"\n",
      "2018-11-15 11:53:53,898 : INFO : topic diff=inf, rho=0.577350\n"
     ]
    }
   ],
   "source": [
    "documents = data['document'].values\n",
    "documents = list(documents)\n",
    "\n",
    "texts = [[word for word in document.lower().split(',')]for document in documents]\n",
    "\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a corpus from a list of texts\n",
    "common_dictionary = Dictionary(texts)\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Train the model on the corpus.\n",
    "lda = gensim.models.LdaModel(common_corpus, num_topics=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-15 15:42:11,233 : WARNING : no word id mapping provided; initializing from corpus, assuming identity\n",
      "2018-11-15 15:42:11,236 : INFO : using symmetric alpha at 0.002\n",
      "2018-11-15 15:42:11,238 : INFO : using symmetric eta at 0.002\n",
      "2018-11-15 15:42:11,239 : INFO : using serial LDA version on this node\n",
      "2018-11-15 15:42:11,344 : INFO : running online (single-pass) LDA training, 500 topics, 1 passes over the supplied corpus of 846 documents, updating model once every 846 documents, evaluating perplexity every 846 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2018-11-15 15:42:11,345 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "2018-11-15 15:42:12,714 : INFO : -490.237 per-word bound, 3768047717749617570868581712182221485267165498968353736191579466599789033711207247203591108119705496123815426803502949649678355691370122842512293888.0 perplexity estimate based on a held-out corpus of 846 documents with 5000 words\n",
      "2018-11-15 15:42:12,715 : INFO : PROGRESS: pass 0, at document #846/846\n",
      "2018-11-15 15:42:13,976 : INFO : topic #197 (0.002): 0.001*\"607\" + 0.001*\"606\" + 0.001*\"611\" + 0.001*\"610\" + 0.001*\"609\" + 0.001*\"608\" + 0.001*\"613\" + 0.001*\"602\" + 0.001*\"604\" + 0.001*\"614\"\n",
      "2018-11-15 15:42:13,980 : INFO : topic #213 (0.002): 0.172*\"569\" + 0.172*\"583\" + 0.172*\"594\" + 0.172*\"581\" + 0.000*\"609\" + 0.000*\"607\" + 0.000*\"611\" + 0.000*\"610\" + 0.000*\"613\" + 0.000*\"604\"\n",
      "2018-11-15 15:42:13,981 : INFO : topic #414 (0.002): 0.323*\"220\" + 0.082*\"0\" + 0.065*\"591\" + 0.046*\"531\" + 0.046*\"172\" + 0.046*\"574\" + 0.042*\"104\" + 0.038*\"554\" + 0.036*\"1\" + 0.036*\"526\"\n",
      "2018-11-15 15:42:13,982 : INFO : topic #87 (0.002): 0.001*\"607\" + 0.001*\"606\" + 0.001*\"611\" + 0.001*\"610\" + 0.001*\"609\" + 0.001*\"608\" + 0.001*\"613\" + 0.001*\"602\" + 0.001*\"604\" + 0.001*\"614\"\n",
      "2018-11-15 15:42:13,983 : INFO : topic #248 (0.002): 0.001*\"607\" + 0.001*\"606\" + 0.001*\"611\" + 0.001*\"610\" + 0.001*\"609\" + 0.001*\"608\" + 0.001*\"613\" + 0.001*\"602\" + 0.001*\"604\" + 0.001*\"614\"\n",
      "2018-11-15 15:42:13,988 : INFO : topic diff=490.339355, rho=1.000000\n"
     ]
    }
   ],
   "source": [
    "# Train the model on the corpus.\n",
    "lda = gensim.models.LdaModel(matrix_1_doc_list, num_topics=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
