{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling Data Preparation\n",
    "\n",
    "reference:\n",
    "* [Beginners Guide to Topic Modeling in Python](https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/)\n",
    "\n",
    "\n",
    "* data format?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#import logging\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "#import tempfile\n",
    "#TEMP_FOLDER = tempfile.gettempdir()\n",
    "#print('Folder \"{}\" will be used to save temporary dictionary and corpus.'.format(TEMP_FOLDER))\n",
    "\n",
    "import gensim\n",
    "\n",
    "from manage_path import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To load entire dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "root_folder = Path('../Data/')\n",
    "file_name = 'TRACE2014_jinming_5000.csv'\n",
    "file_path = root_folder / file_name\n",
    "\n",
    "\n",
    "index_col = ['REC_CT_NB']\n",
    "dtype={'TRC_ST': str, 'BOND_SYM_ID': str, 'CUSIP_ID': str, 'ENTRD_VOL_QT': np.int64, 'RPTD_PR': np.float64 \\\n",
    "       ,'YLD_SIGN_CD': str, 'YLD_PT': np.float64,'ASOF_CD': str, 'Report_Dealer_Index': str \\\n",
    "       ,'Contra_Party_Index': str, 'ISSUE_ID': str, 'OFFERING_AMT':np.int64}\n",
    "\n",
    "parse_dates = ['TRD_EXCTN_DT','EXCTN_TM',['TRD_RPT_DT','TRD_RPT_TM'],'TRD_STLMT_DT','SYSTM_CNTRL_DT' \\\n",
    "               ,'Year','YYYYMM','OFFERING_DATE','MATURITY',['TRD_EXCTN_DT_D','EXCTN_TM_D'],'YYYYQQ']\n",
    "\n",
    "data = pd.read_csv(file_path,dtype=dtype,parse_dates=parse_dates,infer_datetime_format=True \\\n",
    "                   ,converters={'TRD_RPT_TM':lambda x : pd.to_datetime(x,format='%H%M%S')})\n",
    "\n",
    "to_drop = ['TRD_EXCTN_DT','EXCTN_TM','TRD_EXCTN_DT_D','EXCTN_TM_D']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To load only field of interest"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "root_folder = Path('../Data/')\n",
    "#file_name = 'TRACE2014_jinming_5000.csv'\n",
    "file_name = 'TRACE2014_jinming.csv'\n",
    "\n",
    "file_path = root_folder / file_name\n",
    "\n",
    "#field_of_interest_dateOnly = ['BOND_SYM_ID','CUSIP_ID','SCRTY_TYPE_CD','ENTRD_VOL_QT','RPTD_PR','RPT_SIDE_CD','TRD_EXCTN_DT','TRD_RPT_DT','RPT_SIDE_CD','Report_Dealer_Index','Contra_Party_Index']\n",
    "field_of_interest_datetime = ['BOND_SYM_ID','CUSIP_ID','SCRTY_TYPE_CD','ENTRD_VOL_QT','RPTD_PR','RPT_SIDE_CD' \\\n",
    "                              ,'TRD_EXCTN_DT_D','EXCTN_TM_D','TRD_RPT_DT','TRD_RPT_TM', 'Report_Dealer_Index'\\\n",
    "                              ,'Contra_Party_Index','TRC_ST']\n",
    "\n",
    "dtype={'BOND_SYM_ID': str, 'CUSIP_ID': str,'SCRTY_TYPE_CD':str, 'ENTRD_VOL_QT': np.float64, 'RPTD_PR': np.float64 \\\n",
    "       ,'RPT_SIDE_CD':str, 'Report_Dealer_Index': str,'Contra_Party_Index': str, 'TRC_ST':str}\n",
    "\n",
    "parse_dates = {'TRD_RPT_DTTM':['TRD_RPT_DT','TRD_RPT_TM'],'TRD_EXCTN_DTTM':['TRD_EXCTN_DT_D','EXCTN_TM_D']}\n",
    "\n",
    "\n",
    "data = pd.read_csv(file_path,usecols=field_of_interest_datetime,parse_dates=parse_dates\\\n",
    "                   ,infer_datetime_format=True,converters={'TRD_RPT_TM':lambda x : pd.to_datetime(x,format='%H%M%S')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_name):\n",
    "    # Prepare data file path\n",
    "    dataset_directory = get_dataset_directory()\n",
    "    file_path = dataset_directory / file_name\n",
    "    \n",
    "    # Only get the field we want\n",
    "    field_of_interest_datetime = ['BOND_SYM_ID','CUSIP_ID','SCRTY_TYPE_CD','ENTRD_VOL_QT','RPTD_PR','RPT_SIDE_CD' \\\n",
    "                                  ,'TRD_EXCTN_DT_D','EXCTN_TM_D','TRD_RPT_DT','TRD_RPT_TM', 'Report_Dealer_Index'\\\n",
    "                                  ,'Contra_Party_Index','TRC_ST']\n",
    "\n",
    "    data_dtype={'BOND_SYM_ID': str, 'CUSIP_ID': str,'SCRTY_TYPE_CD':str, 'ENTRD_VOL_QT': np.float64, 'RPTD_PR': np.float32 \\\n",
    "           ,'RPT_SIDE_CD':str, 'Report_Dealer_Index': str,'Contra_Party_Index': str, 'TRC_ST':str}\n",
    "\n",
    "    parse_dates = {'TRD_RPT_DTTM':['TRD_RPT_DT','TRD_RPT_TM'],'TRD_EXCTN_DTTM':['TRD_EXCTN_DT_D','EXCTN_TM_D']}\n",
    "    \n",
    "    data = pd.read_csv(file_path, usecols=field_of_interest_datetime, dtype=data_dtype, parse_dates=parse_dates\\\n",
    "                       , infer_datetime_format=True, converters={'TRD_RPT_TM':lambda x : pd.to_datetime(x,format='%H%M%S')})\n",
    "    \n",
    "    # Data Cleaning => keep only Trade Status that is T\n",
    "    data = data[data['TRC_ST'] == 'T']\n",
    "    \n",
    "    # Add new column document_date which is the date of TRD_EXCTN_DTTM\n",
    "    data['document_date'] = data['TRD_EXCTN_DTTM'].dt.date.apply(str)\n",
    "    # Add new column document_buy which is the string representation of report dealer buy on the specific day\n",
    "    data['document_buy'] = data.apply(lambda x: str(x['Report_Dealer_Index'])+ ',' +str(x['document_date'] + ',B') ,axis=1)\n",
    "    # Add new column document_sell which is the string representation of report dealer sell on the specific day\n",
    "    data['document_sell'] = data.apply(lambda x: str(x['Contra_Party_Index'])+ ',' +str(x['document_date'] + ',S') ,axis=1)\n",
    "    \n",
    "    # Get bond_issues\n",
    "    bond_issues_path = dataset_directory / 'Mergent_FISD_Bonds_Issues.csv'\n",
    "    bond_issues_fields = ['ISSUER_ID','COMPLETE_CUSIP']\n",
    "    bond_issues_dtype = {'ISSUER_ID':str , 'COMPLETE_CUSIP':str}\n",
    "    bond_issues = pd.read_csv(bond_issues_path, usecols=bond_issues_fields , dtype=bond_issues_dtype)\n",
    "    \n",
    "    # Get bond_issuers\n",
    "    bond_issuer_path = dataset_directory / 'Mergent_FISD_Bonds_Issuers.csv'\n",
    "    bond_issuer_fields = ['ISSUER_ID', 'AGENT_ID', 'CUSIP_NAME', 'INDUSTRY_GROUP','INDUSTRY_CODE', 'PARENT_ID', 'NAICS_CODE','SIC_CODE']\n",
    "    bond_issuer_dtype = {'ISSUER_ID':str, 'AGENT_ID':str, 'CUSIP_NAME':str, 'INDUSTRY_GROUP':str \\\n",
    "                         ,'INDUSTRY_CODE': str, 'PARENT_ID': str, 'NAICS_CODE':str, 'SIC_CODE':str}\n",
    "    bond_issuer = pd.read_csv(bond_issuer_path, usecols=bond_issuer_fields, encoding='cp1252', dtype=bond_issuer_dtype)\n",
    "    \n",
    "    \n",
    "    #bond_ratings_path = dataset_directory / 'Mergent_FISD_Bonds_Ratings.csv'\n",
    "    #bond_ratings = pd.read_csv(bond_ratings_path)\n",
    "    \n",
    "    \n",
    "    # Merge data with bond issues using complete cusip\n",
    "    data = data.merge(bond_issues, left_on='CUSIP_ID', right_on='COMPLETE_CUSIP', how='left')\n",
    "    # Then, merge data with bond issuers using ISSUER_ID\n",
    "    data = data.merge(bond_issuer, left_on='ISSUER_ID', right_on='ISSUER_ID', how='left')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data('TRACE2014_jinming_5000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name=\"TRACE2014_jinming\"):\n",
    "    print(\"loading data {}...\".format(file_name))\n",
    "    pickle_directory = get_Pickle_directory()\n",
    "    \n",
    "    pickle_file_path = pickle_directory / file_name\n",
    "    \n",
    "    print(\"Getting data from{}...\".format(pickle_file_path))\n",
    "    data = pd.read_pickle(pickle_file_path)\n",
    "    print(\"Data getting success!\")\n",
    "    return data\n",
    "def data_groupby():\n",
    "    data = load_data(file_name='TRACE2014_jinming_5000.pkl')\n",
    "    data_gb_sell = data.groupby(by=['document_sell','BOND_SYM_ID'])\n",
    "    data_gb_buy = data.groupby(by=['document_buy','BOND_SYM_ID'])\n",
    "    return (data_gb_sell,data_gb_buy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data TRACE2014_jinming.pkl...\n",
      "Getting data fromC:\\Users\\raymo\\UMD\\Research\\FINRA_TRACE\\Data\\Pickle\\TRACE2014_jinming.pkl...\n",
      "Data getting success!\n"
     ]
    }
   ],
   "source": [
    "data = load_data(file_name='TRACE2014_jinming.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Document\n",
    "In our model, each document represents the activity of a dealer on a day, which we called it Dealer_Day activity. In each document, there are tokens represented by a bond, or BOND_SYM_ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS TOO SLOW\n",
    "# Add new column document to concatenate Report_Dealer_Index and TRD_RPT_DTTM\n",
    "#data['document'] = data.apply(lambda x: str(x['Report_Dealer_Index'])+ ',' +str(x['TRD_RPT_DTTM'])[:10] ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how to get date of datetime series\n",
    "#str(data['TRD_RPT_DTTM'].dt.date[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Add new column document to concatenate Report_Dealer_Index and TRD_RPT_DTTM\n",
    "data['document_date'] = data['TRD_RPT_DTTM'].dt.date.apply(str)\n",
    "data['document'] = data.apply(lambda x: str(x['Report_Dealer_Index'])+ ',' +str(x['document_date']) ,axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "root_folder = Path('../Data/Pickle/')\n",
    "file_name = 'TRACE2014_jinming'\n",
    "file_path = root_folder / file_name\n",
    "data.to_pickle(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform BOND_SYM_ID to BOND_SYM_ID_transformed with sklearn labelEncoder\n",
    "Because both Gensim and SKlearn require token to be represented by interger ID, we have to encode current to interger ID.<br>\n",
    "Not sure if this is needed for Gensim"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(data['BOND_SYM_ID'])\n",
    "transform = le.transform(data['BOND_SYM_ID'])\n",
    "inverse_transform = le.inverse_transform(transform)\n",
    "print(type(transform))\n",
    "print(type(inverse_transform))\n",
    "\n",
    "# Assign transform data to data['BOND_SYM_ID_transformed'] \n",
    "data['BOND_SYM_ID_transformed'] = transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate if the transform was correct\n",
    "#data['BOND_SYM_ID_inversed_transformed'] = inverse_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make BOND_SYM_ID to dict ---- No need anymore\n",
    "\n",
    "#BOND_SYM_ID_count_dict = data['BOND_SYM_ID'].value_counts().to_dict()\n",
    "#keys = BOND_SYM_ID_count_dict.keys()\n",
    "#np.array(list(keys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group data by ['document','BOND_SYM_ID_transformed'] so that represent a document and token inside it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gb = data.groupby(by=['document','BOND_SYM_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matrix_1 = data_gb['BOND_SYM_ID'].size().unstack(fill_value=0)\n",
    "matrix_1 = matrix_1.sort_index(axis=1)\n",
    "\n",
    "matrix_1_shape = matrix_1.shape\n",
    "print('We have {} documents and {} tokens'.format(matrix_1_shape[0],matrix_1_shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = Path('../Data/Pickle/')\n",
    "file_name = 'Matrix_1'\n",
    "file_path = root_folder / file_name\n",
    "matrix_1.to_pickle(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix 1 => Count of  BOND_SYM_ID in a document"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%timeit \n",
    "# use unstack create sparse matrix\n",
    "matrix_1 = data_gb['BOND_SYM_ID'].size().unstack(fill_value=0)\n",
    "# Make sure column labels are sorted\n",
    "matrix_1 = matrix_1.sort_index(axis=1)\n",
    "\n",
    "matrix_1_shape = matrix_1.shape\n",
    "print('We have {} documents and {} tokens'.format(matrix_1_shape[0],matrix_1_shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use SKlearn Label Encoder to transform tokens into integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(matrix_1.columns)\n",
    "transform = le.transform(matrix_1.columns)\n",
    "inverse_transform = le.inverse_transform(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create id2word to map label encoding to its words then create corpus with gensim Dense2Corpus which takes numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = dict(zip(transform, inverse_transform))\n",
    "\n",
    "matrix1_corpus = gensim.matutils.Dense2Corpus(matrix_1.values,documents_columns=False)\n",
    "# Run LDA in Gensim\n",
    "lda = gensim.models.ldamulticore.LdaMulticore(corpus= matrix1_corpus,id2word=id2word,workers=3, num_topics=500, chunksize=10000, passes=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Faillllllllllllllllll.................\n",
    "\n",
    "# Transform matrix_1 to pd.SparseDataFrame\n",
    "matrix_1 = pd.SparseDataFrame(matrix_1)\n",
    "# Convert matrix_1 to scipy sparse matrix format for Gensim\n",
    "matrix_1 = matrix_1.to_coo()\n",
    "\n",
    "print(matrix_1.todense())\n",
    "\n",
    "# Read matrix_1 into gensim.matutils.Sparse2Corpus\n",
    "matrix1_corpus = gensim.matutils.Sparse2Corpus(matrix_1)\n",
    "\n",
    "# Run LDA in Gensim\n",
    "lda = gensim.models.ldamodel.LdaModel(corpus= matrix1_corpus, num_topics=100, update_every=1, chunksize=10000, passes=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pickle\n",
    "\n",
    "with open('matrix_1_doc_list.txt', 'wb') as fp:\n",
    "    pickle.dump(matrix_1_doc_list, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save matrix_1 dictionary to file\n",
    "#np.save('matrix_1.npy', matrix_1_dict['count']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix 2 => Sum of ENTRD_VOL_QT of a BOND_SYM_ID in document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_2 = data_gb[['ENTRD_VOL_QT']].sum()\n",
    "matrix_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform Matrix 2 data \n",
    "https://scikit-learn.org/stable/modules/preprocessing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_2 = matrix_2.unstack(fill_value=0)\n",
    "matrix_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix 3 => Product of ENTRD_VOL_QT and RPTD_PR of a BOND_SYM_ID in document\n",
    "Create new column QT_X_PR as the product of ENTRD_VOL_QT and RPTD_PR<br>\n",
    "Then do groupby QT_X_PR then sum them to get the total amount of the bond in that document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['QT_X_PR'] = data['ENTRD_VOL_QT'] * data['RPTD_PR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_3 = data_gb[['QT_X_PR']].sum()\n",
    "matrix_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['QT_X_PR'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_3 = matrix_3.unstack(fill_value=0)\n",
    "matrix_3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Corpus for Gensim"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "root_folder = Path('../Data/Pickle/')\n",
    "file_name = 'TRACE2014_jinming_pickle'\n",
    "file_path = root_folder / file_name\n",
    "\n",
    "try:\n",
    "    os.mkdir(root_folder)\n",
    "except OSError:  \n",
    "    print (\"Creation of the directory %s failed\" % root_folder)\n",
    "else:  \n",
    "    print (\"Successfully created the directory %s \" % root_folder)\n",
    "\n",
    "data.to_pickle(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output for SKlearn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def sk_documents(matrix):\n",
    "    documents = {}\n",
    "    document_name = ''\n",
    "    for index, row in matrix.iteritems():\n",
    "        document_name = str(index[0])\n",
    "        if(document_name in documents):\n",
    "            array = documents[document_name]\n",
    "            np.append(array,np.array((index[1],row)))\n",
    "            documents.update({document_name:array})\n",
    "        else:\n",
    "            documents[document_name] = np.array((index[1],row))\n",
    "    documents = documents.values()\n",
    "    documents = np.asarray(list(documents))\n",
    "    return documents\n",
    "\n",
    "matrix_1_doc = sk_documents(matrix_1)\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "# This produces a feature matrix of token counts, similar to what\n",
    "# CountVectorizer would produce on text.\n",
    "lda = LatentDirichletAllocation(n_components=500,random_state=0)\n",
    "lda.fit(matrix_1_doc) \n",
    "# get topics for some given samples:\n",
    "lda.transform(matrix_1_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = data.shape\n",
    "print('We have {} rows {} columns'.format(shape[0],shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_duplicated = data.duplicated().sum()\n",
    "percentage = n_duplicated/shape[0]*100\n",
    "print('{} rows that are entirely the same in the data set which is {:.2f}'.format(n_duplicated,percentage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Number of duplcations based on grouping keys:')\n",
    "test_duplication = [['BOND_SYM_ID'],['CUSIP_ID'],['BOND_SYM_ID','CUSIP_ID'],['Report_Dealer_Index','TRD_EXCTN_DTTM'],\n",
    "                   ['Report_Dealer_Index','TRD_EXCTN_DTTM','BOND_SYM_ID']]\n",
    "for test in test_duplication:\n",
    "    print('{} : {}'.format(test, data.duplicated(subset=test,keep='first').sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.duplicated(keep=False,subset=['Report_Dealer_Index','TRD_EXCTN_DTTM','BOND_SYM_ID'])].sort_values(by=['BOND_SYM_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
